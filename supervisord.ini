[supervisord]
logfile = /data/weblog/supervisord-fastchat.log
logfile_maxbytes=50MB
logfile_backups= 1
loglevel = debug
pidfile = /tmp/supervisord-fastchat.pid
directory=/data/weblog
user=chat


[program:controller]
command=/data/envs/llama/bin/python fastchat/serve/controller.py
environment=TRANSFORMERS_CACHE=/data/llm/models
directory=/data/llm/FastChat

process_name=%(program_name)s_%(process_num)s

stdout_logfile=/data/weblog/controller.log
stdout_logfile_backups= 1
stdout_events_enabled = 1
stderr_logfile=/data/weblog/controller.log
stderr_logfile_backups= 1
stderr_events_enabled = 1

numprocs=1

stopsignal=TERM

autostart=true
autorestart=true

priority = 10


[program:worker]
command=/data/envs/llama/bin/python fastchat/serve/model_worker.py --model-path "lmsys/lmsys/vicuna-13b-delta-v1.1"
;command=/data/envs/llama/bin/python fastchat/serve/model_worker.py --model-path "lmsys/fastchat-t5-3b-v1.0"
environment=TRANSFORMERS_CACHE=/data/llm/models
directory=/data/llm/FastChat
user=chat

process_name=%(program_name)s_%(process_num)s

stdout_logfile=/data/weblog/worker.log
stdout_logfile_backups= 1
stdout_events_enabled = 1
stderr_logfile=/data/weblog/worker.log
stderr_logfile_backups= 1
stderr_events_enabled = 1

numprocs=1

stopsignal=TERM

autostart=true
autorestart=true

priority = 20

[program:web]
command=/data/envs/llama/bin/python fastchat/serve/gradio_web_server.py --model-list-mode reload --add-bard --add-chatgpt --add-claude
environment=TRANSFORMERS_CACHE=/data/llm/models
directory=/data/llm/FastChat

process_name=%(program_name)s_%(process_num)s

stdout_logfile=/data/weblog/web.log
stdout_logfile_backups= 1
stdout_events_enabled = 1
stderr_logfile=/data/weblog/web.log
stderr_logfile_backups= 1
stderr_events_enabled = 1

numprocs=1

stopsignal=TERM

autostart=true
autorestart=true

priority = 30
